{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#import seedir as sd\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import tanh\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.nn import Linear, Module, Dropout\n",
    "\n",
    "from datascience.utils import apply, language, city, brand, group, get_folder\n",
    "from datascience.utils import NotSupportedDataTypeError, NotEqualDataTypeError\n",
    "from datascience.data_loading import load_dataset, load_test_set, one_hot_encoding\n",
    "from datascience.data_loading.load_dataset import assert_equal, assert_argument\n",
    "from datascience.model import DeepLearningModel, MLModel\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Rapport SY23 - Prix d'hotels\n",
    "\n",
    "## 1 - Introduction\n",
    "\n",
    "Le projet consiste en l’étude d’un jeu de données de différents hôtels. En effet, plusieurs plaintes ont été reçues par la répression des fraudes d’un pays imaginaires. Ces plaintes ne concernent exclusivement des consommateurs basés en Europe et ayant réalisé des réservations dans des villes Européennes. L’organisme de répression des fraudes informe qu'elle souhaite concentrer les recherches sur 9 villes car, selon leurs experts métier, elles sont révélatrices du marché de l'hôtellerie en Europe.\n",
    "Afin de pouvoir tester nos différents modèle, une API sera utilisée. Celle-ci permettra via la création d’avatars de pouvoir simuler des recherches d’hôtels d’après plusieurs paramètres (utilisation du téléphone, nationalité de l’avatar, destination et la durée entre le moment de réservation et le début de séjour).  Les requêtes réalisées au moyen de l’API permettront de ressortir une liste d’hôtel avec l’id de celui-ci, son prix et le nombre de chambre disponible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Architecture logicielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Architecture de fichiers\n",
    "\n",
    "Lors de la réalisation de ce projet, nous avons dévelopé un package Python nommé \"datascience\" qui contient l'ensemble des fonctions et classes dont nous avons besoin pour récupérer les données, traiter les données, créer des modèles de Deep learning, les entrainer et les tester. Ce package est lui-même divisé en 5 sous-parties :\n",
    "\n",
    "1. data_loading: Ce package sert à charger les données pour pouvoir les utiliser dans les modèles\n",
    "2. data_recovery: Ce package sert à générer des requetes API pour pouvoir construire un jeu de données\n",
    "3. model: Ce package fournit des classes abstraites pour pouvoir définir des modèles de Machine Learning\n",
    "4. utils: Ce package fournit des outils et données utilisables dans les autres packages\n",
    "5. visualisation: Ce package fournit des fonctions pour pouvoir visualiser le dataset (histogrames)\n",
    "\n",
    "L'architecture complete du package est disponible via le bloc de code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sd.seedir('./datascience', exclude_folders=['__pycache__'], style='emoji')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Architecture des classes\n",
    "\n",
    "Pour mener à bien l'ensemble des tâches nécessaires afin de développer des modèles, nous avons implémenté un certain nombre de classes. Les diagrammes de classes ci-dessous montrent les relations entre les classes. \n",
    "\n",
    "1. La classe Connector:\n",
    "\n",
    "![Connector](./img/inria_classe_connector.png)\n",
    "\n",
    "2. La classe CustomDataset:\n",
    "\n",
    "![CustomDataset](./img/inria_classe_load_DataSet.png)\n",
    "\n",
    "3. La classe Data:\n",
    "\n",
    "![ConDatanector](./img/inria_classe_torch_dataset.png)\n",
    "\n",
    "3. Les classes de modèle:\n",
    "\n",
    "![Model](./img/inria_classe-MLmodel.drawio.png)\n",
    "\n",
    "\n",
    "Ces classes permettent de réaliser l'ensemble des actions pour pouvoir définir et entrainer des modèles de Machine Learning. Cf les parties \"Récuperations des données\", \"Préparation des données\", \"Chargement des données\" et \"Modèles\" pour voir leurs utilisations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Recuperation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Explication de l'API\n",
    "\n",
    "L'API mise à disposition par l'INRIA permet de récupérer des informations sur les hôtels et leurs prix. Les requêtes effectuées vers cette API peuvent contenir les arguments suivants:\n",
    "\n",
    "- city : Nom de la ville où effectuer la recherche d’hôtels. Valeurs : ‘amsterdam’, ‘copenhagen’, ‘madrid’, ‘paris’, ‘rome’, ‘sofia’, ‘valletta’, ‘vienna’ et ‘vilnius’.\n",
    "- date : Nombre de jours entre la date de la requête et le jour requêté (i.e. le nombre de jour entre le moment où vous regardez sur le site et le jour du check-in). Valeurs : Nombres positifs (entre 44 et 0).\n",
    "- language : Langue de l’interface. Valeurs : ‘austrian’, ‘belgian’, ‘bulgarian’, ‘croatian’, ‘cypriot’, ‘czech’, ‘danish’, ‘dutch’, ‘estonian’, ‘finnish’, ‘french’, ‘german’, ‘greek’, ‘hungarian’, ‘irish’, ‘italian’, ‘latvian’, ‘lithuanian’, ‘luxembourgish’, ‘maltese’, ‘polish’, ‘portuguese’, ‘romanian’, ‘slovakian’, ‘slovene’, ‘spanish’ or ‘swedish’.\n",
    "- mobile : Paramètre permettant de simuler une requête réalisée depuis un téléphone portable (au lieu d’un ordinateur). Valeurs : 1 (téléphone) ou 0 (ordinateur).\n",
    "- avatar : Identifiant d’un utilisateur fictif (e.g. un faux compte client créé).\n",
    "\n",
    "Nous avons implémenté un module api_connector pour assurer la communication avec l'API. Ce module contient la classe Connector. \n",
    "Cette classe est instanciée par un constructeur qui prend en argument une clé API unique obtenue auprès des organisateurs.\n",
    "\n",
    "La classe contient les methodes suivantes:\n",
    "- create_avatar : Méthode statique qui fait un appel \"POST\" à l'API pour créer un nouvel utilisateur fictif (autrement dit avatar).\n",
    "- get_avatar: Méthode statique qui fait un appel \"GET\" à l'API et retourne la liste d’avatars déjà disponibles avec leur ids et noms.\n",
    "- query: Méthode statique qui prend en parametre les critères d'une recherche d'un hôtel et qui réalise un appel \"GET\" à l'API qui retourne: la liste des hôtels répondants aux critères demandés (identifiant de l'hôtel, prix, nombre de chambres restantes).\n",
    "\n",
    "Il est important de souligner que une clef API ne peut effectuer que 1000 requêtes par semaine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Réalisation et stockage des requêtes API:\n",
    "\n",
    "Pour pouvoir choisir au mieux les requêtes à effectuer parmi l'ensemble de toutes les requêtes possibles, nous avons implémenté les fonctions suivantes dans le module utils:\n",
    "\n",
    "Génération d'une liste stockant toutes les combinaisons de paramètres (ville, date, langue, mobile) possibles avec l'API  (9\\*45\\*27\\*2 = 21870 requêtes possibles). Ces requêtes sont stockées dans un fichier possible_api_requests.csv. Un parametre \"used\" indique pour chaque requête si elle est déjà utilisée. La fonction qui génère ce fichier s'appelle generate_api_requests et n'a été utilisée qu'une seule fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from datascience.utils.data import language, city\n",
    "api_requests = []\n",
    "for c in city:\n",
    "    for l in language:\n",
    "        for i in range(45):\n",
    "            api_requests.append([c, l, i, 0, 0])\n",
    "            api_requests.append([c, l, i, 1, 0])\n",
    "print(\"10 premiers éléments de la liste:\\n\")\n",
    "pprint.pprint(api_requests[:10])\n",
    "print(f\"\\nLongueur de api_requests => {len(api_requests)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir d'un fichier répertoriant les hôtels de chaque ville (métadonnées fournies par les organisateurs), nous avons décidé que la répartition de nos requêtes entre les villes doit reflèter la répartition des hôtels entre villes. Par exemple, une ville ayant 35% des hôtels, devra avoir 35% des requêtes. Pour cela, nous avons créé un histogramme permettant de visualiser le nombre d'hôtels par ville (la fonction get_distribution) et un pour visualiser le nombre de requêtes à faire par ville (generate_histo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Ce bloc de code affiche la repartition des hôtels parmi les villes.\n",
    "\n",
    "hotels = pd.read_csv('./meta_data/features_hotels.csv')\n",
    "city = hotels[\"city\"].to_numpy()\n",
    "keys = set(city.tolist())\n",
    "total_hotel = len(city)\n",
    "histo = {}\n",
    "\n",
    "\n",
    "for i in keys:\n",
    "    nb_hotel = len(list(filter(lambda x: x == i, city)))\n",
    "    pourcentage = nb_hotel / total_hotel * 100\n",
    "    histo[i] = pourcentage\n",
    "    \n",
    "l = range(len(histo.keys()))\n",
    "plt.bar(l, histo.values(), align='center')\n",
    "plt.xticks(l, histo.keys(),rotation = -25)\n",
    "plt.xlabel(\"City Name\")\n",
    "plt.ylabel(\"Hotel distribution in percent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Ce bloc de code affiche la repartition des requêtes parmi les villes\n",
    "\n",
    "\n",
    "generated_r = pd.read_csv(\"./meta_data/generated_requests.csv\")['city']\n",
    "histo = {}\n",
    "\n",
    "for i in generated_r.to_list():\n",
    "    if i in histo.keys():\n",
    "        histo[i] += 1\n",
    "    else:\n",
    "        histo[i] = 1\n",
    "total = sum(histo.values())\n",
    "data = {'City name': histo.keys(), 'Number of requests': histo.values()}\n",
    "distribution = pd.DataFrame.from_dict(data)\n",
    "distribution['Percentage'] = distribution['Number of requests'] / total\n",
    "distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La réalisation des requêtes est réalisé via le module recovery.py du package data_recovery qui contient les fonctions suivantes :\n",
    "- ```update_dataset()```: Ajoute les réponses d'une requête  au jeu de données.\n",
    "\n",
    "- ```request()```: Cette fonction prend une requête en paramètre et l'execute via la méthode ```query()```. Elle utilise ensuite la fonction ```update_dataset``` pour enregistre le résultat de la requête dans le dataset. Enfin elle modifie les fichiers de métadonnées dans le but de suivre l'évolution des requêtes faites à l'API (par exemple, la mise à jour du paramètre \"used\").\n",
    "\n",
    "- ```take_n_requests()```: Génère aléatoirement les paramètres pour n requêtes en suivant la distribution des requêtes sur les villes.\n",
    "\n",
    "- ```assigning_avatar()```: Crée et affecte un certain nombre d'avatars d'utilisateurs fictifs aux requêtes précedement crées.\n",
    "\n",
    "- ```making_n_requests()```: Appelle l'ensemble des fonctions vues précedement pour effectuer n requêtes.\n",
    "\n",
    "Voici ci dessous l'architecture du jeu de données:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sd.seedir('./dataset', depthlimit=2, itemlimit=3, style='emoji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "file = \"./dataset/amsterdam/amsterdam_austrian.csv\"\n",
    "df = pd.read_csv(file)\n",
    "pd.options.display.max_columns = len(df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Préparation des données\n",
    "\n",
    "Pour preparer les donnees de l'entrainnement et de test de nos modeles, plusieurs implementation ont ete realises:\n",
    "- La classe CustomDatset dans le module load__dataset.py du package data_loading: Une instance de type CustomDataset est instanciée en lui fournissant la matrice x de features indépendantes et le vecteur dépendant y. La classe contient plusieurs méthodes statiques pour gérer les types de ces matrices ainsi qu'une méthode split qui divise le jeu de données en 2: dataset et testset.\n",
    "- La fonction apply() du module utils vise à transformer des features qualitatives en features quantitatives. Pour cela nous associons un identifiant numérique unique à chaque ville, à chaque langue, à chaque marque et à chaque groupe. La marque et le groupe de l'hôtel sont des features que nous avons ajoutées à notre jeu de données en consultant d'autres fichiers fournis par les organisateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pprint.pprint(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La fonction one_hot_encoding() dans le module datascience.data_loading.torch_dataloader vise à encoder les colonnes catégorielles pour arriver a des resultat moins biaisee. L'idée est d'avoir un nombre binaire unique (o ou 1) pour chaque catégorie. Par conséquent, le nombre de chiffres est le nombre de catégories pour l'attribut catégoriel à encoder.\n",
    "- Finalement, La fonction load_dataset() du module load_dataset.py du package data_loading prend comme paramètres le dossier de jeu de données récupéré depuis l'API et le fichier de features supplémentaires, elle les joint et renvoie un objet de type CustomDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('./dataset','./meta_data/features_hotels.csv', dtype=\"pandas\")\n",
    "dataset.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "one_hot_encoding(dataset.x) # dataset.x est la matrice des features indépendantes\n",
    "# Après le one_hot_encoding : explosion du nombre de features (109 au lieu de 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Chargement des données\n",
    "Dans cette partie, nous expliquerons comment ont été chargées les données afin d’être exploiter au mieux par les différents modèles de machine learning (Régression simple, Ridge, Lasso) et deep learning( Deux réseaux de neurones from scratch, un modèle exécuté avec pytorch).\n",
    "\n",
    "#### 4.1 - Module Load_dataset\n",
    "\n",
    "Dans un premier temps, l’enjeux était de créer des classes et fonctions permettant de gérer différents types de dataset et d’unifier les données. Mais également d’associer les données générer par les requêtes avec le fichier détaillant les hotels_features.\n",
    "Ainsi, nous avons créé un module ```load_dataset```, qui sera utile afin de générer les datasets pour les algorithmes de machine learning énumérés précedemment. Dans celui-ci les fonctions suivantes ont été définis :\n",
    "-\tFonction **```assert_equal(x,y)```**, permettant de vérifier si x et y sont du même type,\n",
    "-\tFonction **```assert_argument(x)```**, permettant de vérifier que le dataset x est soit un numpy array soit un dataframe pandas,\n",
    "-\tClasse **```CustomDataset```**, se composant d’un constructeur permettant de vérifier la nature des données grâce aux deux fonctions décrient précedemment et également d’une méthode permettant de diviser les sets X et y en training et test. Pour finir cette classe a également une fonction permettant de convertir les dataframe pandas en numpy array.\n",
    "-\tFonction **```load_dataset(dataset_path, features_hotels, dtype=\"numpy\")```**, qui prend en input le chemin du dataset issue des requêtes, le chemin du dataset avec les features d’hôtels. Cette fonction permettra de concatener le dataset des requêtes avec celui des caractéristiques des hôtels.\n",
    "\n",
    "Ainsi, fonction ```load_dataset``` sera utilisée et permettra d’avoir donc deux datasets un pour les features :\n",
    "-\tCity,\n",
    "-\tdate,\n",
    "-\tlanguage,\n",
    "-\tmobile,\n",
    "-\tstock,\n",
    "-\tgroup,\n",
    "-\tbrand,\n",
    "-\tparking,\n",
    "-\tpool,\n",
    "-\tchildren_policy,\n",
    "-\torder_requests\n",
    "\n",
    "Et un dataset Y avec les prix associés afin de pouvoir effectuer les algorithmes de regression comme vu ci-après:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    \"\"\" A class to handle dataset using numpy array or pandas Dataframe\n",
    "\n",
    "    :param x: The features of the dataset\n",
    "    :param y: The output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        assert_equal(x, y)\n",
    "        assert_argument(x)\n",
    "        assert_argument(y)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def getsize(self):\n",
    "        \"\"\" Calculate the length of the data set\n",
    "\n",
    "        :return: The length\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def split(self, dist=[0.8, 0.2]):\n",
    "        \"\"\" Split the data set in multiple subset\n",
    "\n",
    "        If n is the number of subset, the list dist should contain n-1 values\n",
    "\n",
    "        :param dist: A list containg the size of output subset\n",
    "        :return: A tuple containing all subset\n",
    "        \"\"\"\n",
    "        dist = list(map(lambda x: int(self.getsize() * x), dist))\n",
    "        for i in range(1, len(dist)):\n",
    "            dist[i] += dist[i - 1]\n",
    "        result = []\n",
    "        if isinstance(self.x, pd.core.frame.DataFrame):\n",
    "            dist = [0] + dist + [self.getsize()]\n",
    "\n",
    "            for i in range(1, len(dist)):\n",
    "                x = self.x.iloc[dist[i - 1]:dist[i], :]\n",
    "                y = self.y.iloc[dist[i - 1]:dist[i], :]\n",
    "                x.reset_index(drop=True, inplace=True)\n",
    "                y.reset_index(drop=True, inplace=True)\n",
    "                result.append(CustomDataset(x, y))\n",
    "        elif isinstance(self.x, np.ndarray):\n",
    "            x_split = np.array_split(self.x, dist)\n",
    "            y_split = np.array_split(self.y, dist)\n",
    "\n",
    "            for i in range(len(x_split)):\n",
    "                result.append(CustomDataset(x_split[i], y_split[i]))\n",
    "\n",
    "        return tuple(result)\n",
    "\n",
    "    def to_numpy(self):\n",
    "        if isinstance(self.x, pd.core.frame.DataFrame):\n",
    "            self.x = self.x.to_numpy()\n",
    "            self.y = self.y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, features_hotels, dtype=\"numpy\"):\n",
    "    \"\"\" A function to load the dataset directory\n",
    "\n",
    "    :param dataset_path: The path of the dataset directory\n",
    "    :param features_hotels: The path of the file containing features of each hotel\n",
    "    :param dtype: The output data type of the function\n",
    "    :return: An object containing the dataset in the data type specified in the dtype parameter\n",
    "    :rtype: CustomDataset\n",
    "    \"\"\"\n",
    "    city_folder = get_folder(dataset_path)\n",
    "    rows = None\n",
    "    for i in city_folder:\n",
    "        language_file = get_folder(f\"{dataset_path}/{i}\")\n",
    "        for j in language_file:\n",
    "            temp = pd.read_csv(f\"{dataset_path}/{i}/{j}\")\n",
    "            if rows is None:\n",
    "                rows = temp.to_numpy()\n",
    "            else:\n",
    "                rows = np.concatenate((rows, temp.to_numpy()))\n",
    "\n",
    "    np.random.shuffle(rows)\n",
    "    rows = pd.DataFrame(rows,\n",
    "                        columns=['hotel_id', 'price', 'stock', 'city', 'date', 'language', 'mobile', 'avatar_id',\n",
    "                                 'order_requests'])\n",
    "    hotels = pd.read_csv(features_hotels, index_col=['hotel_id', 'city'])\n",
    "    pricing_requests = rows.join(hotels, on=['hotel_id', 'city'])\n",
    "    y_data_set = pricing_requests[['price']]\n",
    "    x_data_set = pricing_requests[[\n",
    "        'city',\n",
    "        'date',\n",
    "        'language',\n",
    "        'mobile',\n",
    "        'stock',\n",
    "        'group',\n",
    "        'brand',\n",
    "        'parking',\n",
    "        'pool',\n",
    "        'children_policy',\n",
    "        'order_requests'\n",
    "    ]]\n",
    "    x_data_set = x_data_set.applymap(apply)\n",
    "    if dtype == \"numpy\":\n",
    "        return CustomDataset(x_data_set.to_numpy(), y_data_set.to_numpy())\n",
    "    elif dtype == \"pandas\":\n",
    "        return CustomDataset(x_data_set, y_data_set)\n",
    "    else:\n",
    "        raise \"Wrong data type\"\n",
    "        \n",
    "dataset = load_dataset('./dataset','./meta_data/features_hotels.csv', dtype=\"pandas\")\n",
    "print(dataset.x)\n",
    "print(dataset.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - DataLoader Pytorch\n",
    "\n",
    "Dans un second temps, nous avons eu besoin d’utiliser pytorch. Un nouveau package a donc été créée. En effet, la manière de charger les données avec pytorch est assez différente. Nous avons deux objets : DataSet and DataLoader objects.\n",
    "Avant de détailler le fonctionnement d’un DataLoader pytorch, notons qu’il a été créé une fonction one_hot_encoding(x) qui permettra de faire du one hot encoding sur les caractéristiques suivantes :\n",
    "-\tCity,\n",
    "-\tLanguage,\n",
    "-\tBrand,\n",
    "-\tGroup,\n",
    "-\tChildren,\n",
    "-\tDate,\n",
    "\n",
    "La classe DataSet utilisera la fonction load_dataset précédemment définie et ainsi transforma les deux numpy array X et y, correspondant respectivement aux features et aux prix des différents hôtels en tensors mais également de définir au moyen de la méthode get_item différents sample de notre dataset et d’ainsi améliorer le training. \n",
    "En effet, faire de nombreuses itérations sur le datset est très couteux en temps. C’est pourquoi, la classe DataSet permettra de diviser le dataset en plusieurs batch qui seront regrouper pour faire des batch. Pour être plus claire, nous allons définir certains termes.\n",
    "-\tUne epoch correspond à un forward et backward pass pour tous les trainings batch,\n",
    "-\tUn batch correspond à une combinaison de sample du dataset,\n",
    "-\tLe batch size est donc le nombre de sample dans un forward et backward pass,\n",
    "-\tLe nombre d’itérations est le nombre de boucle faite sur les différents batch\n",
    "\n",
    "**Exemple :**\n",
    "Samples=100,\n",
    "Batch_size=20,\n",
    "Nombre d’itérations=100/20=5 pour une epoch\n",
    "\n",
    "Ainsi, la fonction prepar_data_loader permet d’appeler la classe DataLoader qui divisera les données en différents batch de sample de données et qui fera en sorte d’optimiser le training avec l’option shuffle et num_workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, dataset_path, features_hotels, dtype=\"onehot\"):\n",
    "        dataset = load_dataset(dataset_path, features_hotels, dtype=\"pandas\")\n",
    "\n",
    "        if dtype == \"matrix\":\n",
    "            x = one_hot_encoding(dataset.x)\n",
    "            x = to_matrix(x)\n",
    "            self.X = torch.from_numpy(x.astype(np.float32))\n",
    "        elif dtype == \"onehot\":\n",
    "            x = one_hot_encoding(dataset.x)\n",
    "            self.X = torch.from_numpy(x.to_numpy().astype(np.float32))\n",
    "        elif dtype == \"relative\":\n",
    "            x = relative_column(dataset.x)\n",
    "            self.X = torch.from_numpy(x.to_numpy().astype(np.float32))\n",
    "        else:\n",
    "            raise f\"{dtype} => Invalid type for loading test set\"\n",
    "\n",
    "        self.y = torch.from_numpy(dataset.y.to_numpy().astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataloader(dataset_path, features_hotels, dist=[0.8, 0.2], batch_size=64, dtype=\"onehot\"):\n",
    "    dataset = Data(dataset_path, features_hotels, dtype)\n",
    "    rep = list(map(lambda x: int(x * dataset.__len__()), dist))\n",
    "    rep[-1] += dataset.__len__() - sum(rep)\n",
    "\n",
    "    split = torch.utils.data.random_split(dataset, rep)\n",
    "    data = []\n",
    "\n",
    "    for i in split:\n",
    "        data.append(DataLoader(dataset=i, batch_size=batch_size, shuffle=True, drop_last=True))\n",
    "\n",
    "    return tuple(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 - Test set\n",
    "\n",
    "Chaque jour, nous avons le droit à un nombre limité de soumissions (5). Ces sousmissions sont faites à partir d'un jeu de données fourni par l'INRIA, et il suffit d'appliquer un modèle de regression sur l'ensemble de ce jeu de données. La fonction load_test_set se trouvant dans le package load_test_set permet de charger le jeu de données de soumission et de le concaténer avec les features des hotels et ensuite d'éffectuer les modifications nécessaires afin de pouvoir les manipuler avec pytorch et de faire les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def load_test_set(test_set='meta_data/test_set.csv', features_hotels='meta_data/features_hotels.csv'):\n",
    "    to_predict = pd.read_csv(test_set)\n",
    "    hotels = pd.read_csv(features_hotels, index_col=['hotel_id', 'city'])\n",
    "    to_predict = to_predict.join(hotels, on=['hotel_id', 'city'])\n",
    "    to_predict = to_predict.applymap(apply)\n",
    "\n",
    "    return to_predict['index'], to_predict[[\n",
    "        'city',\n",
    "        'date',\n",
    "        'language',\n",
    "        'mobile',\n",
    "        'stock',\n",
    "        'group',\n",
    "        'brand',\n",
    "        'parking',\n",
    "        'pool',\n",
    "        'children_policy',\n",
    "        'order_requests'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from datascience.data_loading.torch_dataloader import one_hot_encoding,relative_column,to_matrix\n",
    "\n",
    "def torch_test_set(test_set='meta_data/test_set.csv', features_hotels='meta_data/features_hotels.csv', dtype='onehot'):\n",
    "    index, test_set = load_test_set(test_set, features_hotels)\n",
    "\n",
    "    if dtype == 'onehot':\n",
    "        test_set = one_hot_encoding(test_set)\n",
    "        return index, test_set\n",
    "    elif dtype == 'matrix':\n",
    "        test_set = one_hot_encoding(test_set)\n",
    "        return index, to_matrix(test_set)\n",
    "    elif dtype == 'relative':\n",
    "        return index, relative_column(test_set)\n",
    "    else:\n",
    "        raise f\"{dtype} => Invalid type for loading test set\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch_test_set()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Les modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Modèles from scrach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 - Modèle sous forme de fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def initialisation(dimensions):\n",
    "    parametres = {}\n",
    "    C = len(dimensions)\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    for c in range(1, C):\n",
    "        parametres['W' + str(c)] = np.random.randn(dimensions[c], dimensions[c - 1])\n",
    "        parametres['b' + str(c)] = np.random.randn(dimensions[c], 1)\n",
    "\n",
    "    return parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parametres):\n",
    "    activations = {'A0': X}\n",
    "\n",
    "    # C est le nombre de couches si on a 4 paramètres cela signifie qu'on a deux couches\n",
    "    C = len(parametres) // 2\n",
    "\n",
    "    for c in range(1, C):\n",
    "        Z = parametres['W' + str(c)].dot(activations['A' + str(c - 1)]) + parametres['b' + str(c)]\n",
    "        Z = Z.astype(np.float64)\n",
    "        activations['A' + str(c)] = 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    # print(activations)\n",
    "    Z = parametres['W' + str(C)].dot(activations['A' + str(C - 1)]) + parametres['b' + str(C)]\n",
    "    Z = Z.astype(np.float64)\n",
    "    activations['A' + str(C)] = Z\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def back_propagation(y, parametres, activations):\n",
    "    m = y.shape[1]\n",
    "    C = len(parametres) // 2\n",
    "\n",
    "    dZ = activations['A' + str(C)] - y\n",
    "    gradients = {}\n",
    "\n",
    "    for c in reversed(range(1, C + 1)):\n",
    "        gradients['dW' + str(c)] = 1 / m * np.dot(dZ, activations['A' + str(c - 1)].T)\n",
    "        gradients['db' + str(c)] = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        if c > 1:\n",
    "            dZ = np.dot(parametres['W' + str(c)].T, dZ) * activations['A' + str(c - 1)] * (\n",
    "                    1 - activations['A' + str(c - 1)])\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def update(gradients, parametres, learning_rate):\n",
    "    C = len(parametres) // 2\n",
    "\n",
    "    for c in range(1, C + 1):\n",
    "        parametres['W' + str(c)] = parametres['W' + str(c)] - learning_rate * gradients['dW' + str(c)]\n",
    "        parametres['b' + str(c)] = parametres['b' + str(c)] - learning_rate * gradients['db' + str(c)]\n",
    "\n",
    "    return parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, parametres):\n",
    "    activations = forward_propagation(X, parametres)\n",
    "    C = len(parametres) // 2\n",
    "    Af = activations['A' + str(C)]\n",
    "    return Af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def prep_data(X, y):\n",
    "    X_train_transpose = X.T\n",
    "    Y_train_transpose = y.T\n",
    "    return X_train_transpose, Y_train_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def deep_neural_network(X, y, hidden_layers=(16, 16, 16), learning_rate=0.001, n_iter=3000):\n",
    "    X, y = prep_data(X, y)\n",
    "    # initialisation parametres\n",
    "    dimensions = list(hidden_layers)\n",
    "    # On insert les nombres de coordonnées pour un individu\n",
    "    dimensions.insert(0, X.shape[0])\n",
    "    # On donne une sortie\n",
    "    dimensions.append(y.shape[0])\n",
    "    np.random.seed(1)\n",
    "    parametres = initialisation(dimensions)\n",
    "\n",
    "    # tableau numpy contenant les futures accuracy et log_loss\n",
    "    training_history = np.zeros((int(n_iter), 2))\n",
    "\n",
    "    C = len(parametres) // 2\n",
    "\n",
    "    # gradient descent\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        activations = forward_propagation(X, parametres)\n",
    "\n",
    "        gradients = back_propagation(y, parametres, activations)\n",
    "        parametres = update(gradients, parametres, learning_rate)\n",
    "        Af = activations['A' + str(C)]\n",
    "        #print(Af)\n",
    "        # calcul du log_loss et de l'accuracy\n",
    "        # print(y.flatten().astype(np.int32).shape)\n",
    "        # print(Af.flatten().shape)\n",
    "        #training_history[i, 0] = (mean_squared_error(y.flatten().astype(np.int32), Af.flatten(), squared=False))\n",
    "        y_pred = predict(X, parametres)\n",
    "        #print(y_pred)\n",
    "        # print(y_pred.flatten().shape)\n",
    "        # print(y.flatten().astype(np.int32).shape)\n",
    "        # training_history[i, 1] = (mean_squared_error(y.flatten().astype(np.int32), y_pred.flatten(), squared=False))\n",
    "\n",
    "    #y_pred = predict(X, parametres)\n",
    "    #print(y_pred)\n",
    "    # Plot courbe d'apprentissage\n",
    "    #plt.plot(training_history[:, 0], label='train loss')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    rmse=mean_squared_error(y.flatten().astype(np.int32), Af.flatten(), squared=False)\n",
    "    print(rmse)\n",
    "    return parametres,rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from datascience.model import MLModel\n",
    "class AmbreNet(MLModel):\n",
    "    def __init__(self, dataset='./dataset', features_hotels='./meta_data/features_hotels.csv'):\n",
    "        super().__init__(dataset=dataset, features_hotels=features_hotels)\n",
    "        self.parametres = None\n",
    "        self.rmse=None\n",
    "        #self.dataset.x = one_hot_encoding(self.dataset.x)\n",
    "        self.dataset.to_numpy()\n",
    "        index = int(0.05 * len(self.dataset.x))\n",
    "        self.dataset.x = self.dataset.x[:index]\n",
    "        self.dataset.y = self.dataset.y[:index]\n",
    "\n",
    "    def train(self, hidden_layers=(16,16,16), learning_rate=0.01,n_iter=10):\n",
    "        self.parametres,self.rmse = deep_neural_network(self.dataset.x, self.dataset.y, hidden_layers=hidden_layers,\n",
    "                                              learning_rate=learning_rate,\n",
    "                                              n_iter=n_iter)\n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        return predict(x, self.parametres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vect_layers=np.linspace(4,24,6,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rmse_scratch=[]\n",
    "for i in vect_layers:\n",
    "    model=AmbreNet()\n",
    "    model.train(hidden_layers=(i,i,i),learning_rate=0.01,n_iter=2)\n",
    "    rmse_scratch.append(model.rmse)\n",
    "    print(model.rmse)\n",
    "#plt.plot(vect_layers,rmse_scratch)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(vect_layers,rmse_scratch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = AmbreNet()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 - Modèle sous forme de classe\n",
    "\n",
    "Pour donner suite à l'implementation from scractch de réseaux de neurones en utilisant des fonctions (cf 4.1.1) comme nous avons vu en TP et en cours. Nous avons décidé de développé un packages python permettant d'implementer des réseaux de neuronnes comme on le ferait si on utilisait un framework python comme Pytorch ou keras/tensorflow. \n",
    "\n",
    "Ce package se base sur l'utilisation de la programation orientée objet classique (classe, heritage, polymorphisme, etc..) ainsi que sur l'utilisation de patrons de conception (Streategy) pour permettre une plus grande modularité du code.\n",
    "\n",
    "On a identifié différents elements necessaires pour pouvoir implementer un réseau de neurones (ci-apres):\n",
    "1. Fonction d'activation\n",
    "2. Fonction d'erreur\n",
    "3. Méthode d'optimisation\n",
    "4. Le type de couche\n",
    "5. L'apprentissage (backpropagation)\n",
    "\n",
    "On a donc défini des sous-packages pour chaque element. Voici l'architecture global du package (UTTnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sd.seedir('./UTTnet', exclude_folders=['__pycache__'], style='emoji')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque sous-package fonctionne sur le même principe. Pour comprendre comment fonctionne le package UTTnet il faut comprendre comment fonctionne le patron de conception statregie dont voici le diagramme UML.\n",
    "\n",
    "![Strategy](./img/Strategy_Pattern_in_UML.png)\n",
    "\n",
    "Ce patron de conception permet de garantir la modularité du code en permettant l'implementation de nouveaux types d'objets tant qu'ils respectent certaines conditions (heritage une interface).\n",
    "\n",
    "Dans le cas de ce package, nous avons adapté le patron de conception pour qu'il réponde à nos besoins. Nous allons parcourir les sous-packages dans l'odre pour expliquer leurs fonctionnement et leurs buts:\n",
    "\n",
    "1. Metric: Ce sous-packages contient les classes pour implemnter à la fois les fonction d'erreurs et les metriques d'évaluation.\n",
    "\n",
    "![Metric](./img/uttnet-metric.png)\n",
    "\n",
    "2. Activation: Ce sous-package contient les classes necessaires pour implemnter différentes fonctions d'activations. Les classes sont utilisées à la fois pour la forward et la backward pass.\n",
    "\n",
    "![Activation](./img/uttnet-Activation.png)\n",
    "\n",
    "3. Layer: Ce sous-package contient l'ensemble des classes pour implementer différents types de couches. \n",
    "\n",
    "![Layer](./img/uttnet-Layer.png)\n",
    "\n",
    "4. Optimizer: Ce sous-package contient les classes permettant de réaliser l'optimisation des poids des couches selon différentes méthodes.\n",
    "\n",
    "![Optimizer](./img/uttnet-optim.png)\n",
    "\n",
    "\n",
    "Maintenant que l'on a défini l'ensemble des sous packages, on peut representer la classe Network qui rassemble l'ensemble des classes vues précedement pour pouvoir implementer un réseau de neurones complet.\n",
    "\n",
    "![Network](./img/uttnet-Network.png)\n",
    "\n",
    "Il suffit d'instancier la classe Network puis de ajouter des couches avec la méthode add pour définir un réseau de neurones. Voici un exemple simple de réseau de neurones utilisant ce package en modelisant un xor. On commence par tester sur un probleme xor, car c'est un probleme tres simple et cela permet de vérifier que le package fonctionne bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from UTTnet import Network\n",
    "from UTTnet.Activation import Tanh\n",
    "from UTTnet.Metric import Mse\n",
    "from UTTnet.Layer import Dense\n",
    "from UTTnet.Optimizer import SGD\n",
    "\n",
    "# We are going to test out module on a xor problem\n",
    "\n",
    "x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# Define the network\n",
    "# Loss function is Mean Squared Error\n",
    "# Optimizer is Stochastic Gradient Descent\n",
    "model = Network(\n",
    "    loss=Mse(),\n",
    "    optimizer=SGD(learning_rate=0.1)\n",
    ")\n",
    "\n",
    "# Add two layers\n",
    "# First layer 2 input and 3 output using tanh activation function and bias is set to true\n",
    "model.add(Dense(2, 3, activation=Tanh()))\n",
    "# Second layer 3 input and 1 output using tanh activation function and bias is set to true\n",
    "model.add(Dense(3, 1, activation=Tanh()))\n",
    "\n",
    "# Print the model\n",
    "print(model)\n",
    "\n",
    "# Train the model\n",
    "loss = model.fit(x_train, y_train, epochs=1000)\n",
    "# test the model on\n",
    "out = model(x_train)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from UTTnet import Network\n",
    "from UTTnet.Activation import Tanh,Relu\n",
    "from UTTnet.Metric import Mse\n",
    "from UTTnet.Layer import Dense\n",
    "from UTTnet.Optimizer import SGD\n",
    "from datascience.data_loading import one_hot_encoding\n",
    "from datascience.data_loading import load_dataset\n",
    "dataset = load_dataset('./dataset','./meta_data/features_hotels.csv', dtype=\"pandas\")\n",
    "dataset.x = one_hot_encoding(dataset.x)\n",
    "dataset.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_2 = Network(\n",
    "    loss=Mse(),\n",
    "    optimizer=SGD()\n",
    ")\n",
    "\n",
    "model_2.add(Dense(109,109,activation=Tanh()))\n",
    "model_2.add(Dense(109,1))\n",
    "\n",
    "\n",
    "loss = model_2.fit(dataset.x[:1500], dataset.y[:1500], epochs=5)\n",
    "# test the model on\n",
    "out = model_2(dataset.x[2000])\n",
    "print(f\"x = {dataset.x[2000]}\")\n",
    "print(f\"prediction = {out}\")\n",
    "print(f\"true = {dataset.y[2000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Modèles de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 - Regression lineaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre problème, on a un ensemble de données sous la forme d'entrées et de sorties correspondantes, nous sommes donc dans un apprentissage supervisé. De plus, la sortie qu'on cherche est une valeur numérique, notre problème est un problème de régression. Dans notre situation on souhaiterait faire une généralisation sous la forme d'une fonction linéaire, la méthode à tester est donc : la régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "\n",
    "dataset = load_dataset('dataset/', 'meta_data/features_hotels.csv', dtype=\"numpy\")\n",
    "\n",
    "index = int(0.2 * len(dataset.x))\n",
    "dataset.x = dataset.x[:index]\n",
    "dataset.y = dataset.y[:index]\n",
    "        \n",
    "train_set, valid_set = dataset.split(dist=[0.90])\n",
    "\n",
    "model.fit(train_set.x,train_set.y)\n",
    "\n",
    "y_predicted=[]\n",
    "for i in valid_set.x:\n",
    "    prediction = model.predict([i])\n",
    "    y_predicted.append(prediction[0])\n",
    "rmse = mean_squared_error(valid_set.y, y_predicted, squared=False)\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 - Regression Poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que l' erreur quadratique moyenne prend une grande valeur (55.15) pour la régression linéaire. Avant de se lancer dans la recherche d'un algorithme d'apprentissage artificiel, testons la régression polynomiale. Dans le bloc de code suivant, nous calculons le RMSE de la régression polynomiale en fonction du degré choisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Regression(MLModel):\n",
    "    def __init__(self, degree, dataset='dataset/', features_hotels='meta_data/features_hotels.csv'):\n",
    "        super().__init__(dataset, features_hotels)\n",
    "        self.dataset.to_numpy()\n",
    "        index = int(0.2 * len(self.dataset.x))\n",
    "        self.dataset.x = self.dataset.x[:index]\n",
    "        self.dataset.y = self.dataset.y[:index]\n",
    "        self.train_set, self.valid_set = self.dataset.split(dist=[0.98])\n",
    "        self.model = linear_model.LinearRegression()\n",
    "        self.poly_model = PolynomialFeatures(degree)\n",
    "\n",
    "    def train(self):\n",
    "        poly_x_train = self.poly_model.fit_transform(self.train_set.x)\n",
    "        self.model.fit(poly_x_train, self.train_set.y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        poly_x_predict= self.poly_model.fit_transform([x])\n",
    "\n",
    "        return self.model.predict(poly_x_predict)[0]\n",
    "\n",
    "    def validate(self):\n",
    "        return super().validate(self.valid_set.x, self.valid_set.y)\n",
    "\n",
    "\n",
    "rmse=[]\n",
    "for i in range(1,5):\n",
    "    model = Regression(i)\n",
    "    model.train()\n",
    "    rmse.append(model.validate())\n",
    "\n",
    "plt.title(\"RMSE en fonction du degree de la regression polynomiale\")\n",
    "\n",
    "plt.plot(range(1,5), rmse, color=\"blue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figure précédente montre que le RMSE de la régression polynomiale est décroissant en fonction du degré choisi. C'est une méthode performante en terme de précision en augmentant le degré, mais malheureusement elle demande une grande complexité temporelle à cause de l'explosion exponentielle des paramètres. Ce qui a demandé la construction d'un autre algorithme qui demande une moindre complexité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 - Regression Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 - Regression Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "dataset = load_dataset('./dataset','./meta_data/features_hotels.csv', dtype=\"pandas\")\n",
    "\n",
    "def lass(x_set,y_set,percentage,fold):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_set, y_set, test_size=percentage, random_state=10)\n",
    "    # We are using LassoCV with 5 folds but we could also try with 10 folds\n",
    "    model = LassoCV(cv=fold, random_state=0, max_iter=10000)\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    # We are using the best value of alpha \n",
    "    lasso_best = Lasso(alpha=model.alpha_)\n",
    "    lasso_best.fit(X_train, y_train)\n",
    "    #Here are the coeff of the lasso regression per metrics\n",
    "    #print(list(zip(lasso_best.coef_, x_set)))\n",
    "    #Compute the RMSE\n",
    "    rmse = mean_squared_error(y_test,  lasso_best.predict(X_test), squared=False)\n",
    "    return rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vect_para_lass_1=np.linspace(0.1,0.3,6)\n",
    "vect_para_lass_2=np.linspace(5,20,4,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vect_para_lass_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vect_para_lass_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lass(dataset.x,dataset.y,0.3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rmse_list=[]\n",
    "for i in vect_para_lass_1:\n",
    "    rmse=lass(dataset.x,dataset.y,i,10)\n",
    "    rmse_list.append(rmse)\n",
    "plt.plot(vect_para_lass_1,rmse_list)    \n",
    "plt.legend()\n",
    "plt.show()\n",
    "#lass(x_data_set,y_data_set,0.2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rmse_list=[]\n",
    "for i in vect_para_lass_2:\n",
    "    rmse=lass(dataset.x,dataset.y,0.2,i)\n",
    "    rmse_list.append(rmse)\n",
    "plt.plot(vect_para_lass_2,rmse_list)    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Modèles pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class NNModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = Linear(109, 109)\n",
    "        self.output = Linear(109, 1)\n",
    "        self.dropout = Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = tanh(self.input(x))\n",
    "        return self.output(x)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "\n",
    "epochs=10\n",
    "model = NNModel()\n",
    "nn = DeepLearningModel(model)\n",
    "loss_values, val_loss_values = nn.train(epochs=epochs, learning_rate=0.001, batch_size=16, show=True)\n",
    "x = list(range(1, epochs + 1))\n",
    "plt.plot(x, loss_values, color='b', label='train')\n",
    "plt.plot(x, val_loss_values, color='r', label='validation')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## 5 - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>Pour conclure sur ce projet on va commencer par rappeler les objectifs initiaux du projet. Pour rappel, ce projet est basé sur le challenge « challenge IA 2023 » disponible sur la plateforme Kaggle et organisé par l’INRIA. Ce challenge traite du prix des chambres d’hôtels dans plusieurs villes européennes dans le but de détecter les hôtels proposant des prix frauduleux en fonction des clients. Ce challenge est donc un problème de régression où il faut effectuer des estimations de prix de chambres d’hôtels en se basant sur un certain nombre de features (en relation avec les clients et les hôtels). La particularité de ce challenge est qu’il n’y a pas de jeu de données fourni par l’organisateur et il faut donc le construire nous-mêmes grâce à une API fournie par l’INRIA. Cette API permet d’obtenir des prix de chambres d’hôtels en réalisant des requêtes qui correspondent à des fausses réservations de chambre d’hôtels. Pour éviter que les hôtels ne se rendent compte que l’on effectue une enquête, l’INRIA nous limite à 1000 requêtes par semaine. La difficulté du premier aspect du challenge était de faire en sorte que le jeu de données que l’on construit soit représentatif de la population des prix de chambres d’hôtels. La deuxième tache que l’on devait réaliser dans le cadre de ce challenge était le développement de modèles de régression pour effectuer les estimations des prix de chambres d’hôtels. Ces modèles de régression ont été entrainé sur le jeu de données que l’on a précédemment construit. Ces modèles dépendent grandement de la qualité du jeu de données. L’INRIA a mis à disposition un jeu de données spécial appelé « test set » qui est composé de l’ensemble des features et sur lequel on doit pour chaque ligne faire une estimation du prix de la réservation. Ce jeu de données a pour but d’avoir une base commune entre compétiteurs pour pouvoir comparer nos modèles respectifs. On doit ensuite déposer le fichier contenant l’ensemble des estimations sur Kaggle pour obtenir un score.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour développer le meilleur modèle possible pour résoudre ce challenge, nous avons testé plusieurs approches en appliquant les méthodes de machine Learning et de deep Learning suivantes :\n",
    "\n",
    "- Regression Linéaires et polynomiales\n",
    "- Regression de Lasso et Ridge\n",
    "- Réseau de neurones from scratch et en utilisant Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tableau ci-dessous résume les performance de chaque modèle à la fois sur le jeu de données que nous avons construit et sur le jeu de données fournit par l'INRIA pour éffectuer nos sousmissions.\n",
    "\n",
    "|                     | Régression Lineaire | Régression Polynomiale | Régression Lasso | Régression Ridge | Réseau de neurone from scratch | Réseau de neurone Pytorch |\n",
    "| :-----------------: | :----------: | :----------: | :----------: | :----------: | :----------: | :----------: |\n",
    "| **Test set**        | test | test | test | test | test | test | \n",
    "| **Submission test** | test | test | test | test | test | test |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expliquer la différence de résultat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
